# Оценка производительности библиотек.

У всех, и у меня в том числе, пригорает от перформанс тестов и выводов, которые делаются на их основе. В [недавнем посте](https://t.me/sergeysova/445) и треду к нему идут рассуждения о том, как правильно и неправильно делать оценку производительности. В нем я немного расписываю мотивацию, зачем вообще этим заниматься, и теперь мне хочется подвести какой-то итог со своей стороны в рамках этой статьи. 

Несмотря на то, что у меня не так много опыта в вопросе тестирования производительности, я все-таки хотел бы поделится некоторыми своими мыслями.

Абсолютно объективную оценку производительности невозможно дать из-за разнообразия железа, на котором запускается код. Мощность железа нельзя уровнять по какому-то единственному попугаю, разные инструкции могут исполняться быстрее или медленнее или отсутствовать вовсе в процессоре. Примерно тоже самое касается и софта, который компилируется от версии к версии в разные наборы инструкций.

Само по себе измерение — это вычислительная операция, которая в шесть девяток случаев происходит в среде исполнения измеряемого кода. Код, производящий измерение влияет прямо или косвенно на производительность замеряемого кода. В основном, это связано с переиспользованием памяти, но могут быть и приколюхи от компилятора, который подвигал инструкции в не совсем ожидаемом порядке.

Для получения стабильного результата тестов необходимо, внезапно, получить стабильный результат каждого теста. Для этого тест повторятся пока он не станет выполняться за одно и тоже время от раза к разу. К сожалению, это не эффективный способ при измерении кода, который выполняется с JIT компилятором и оптимизатором. Поэтому меньшим из зол является оценка медианных значений и отсечки по перцентилям (я предпочитаю 5%). На всякие случай уточню: среднее арифметическое вообще выстрел в небо.

Учитывая предыдущий пункт, количество измерений производительности выбирается и задается вручную, что уже не соотносится с формальными правилами измерений, полноное отключение JIT также не будет соответствовать реальному использованию кода, поэтому такой тест будет бесполезным. В зависимости от назначения измеряемого кода необходимо определить примерное количество запусков в реальном приложении, а после брать медиану.

Код теста производительности — это одно из самых спорных мест, частое место преткновений. Код должен быть достаточно функциональным и похожим на решение реальной задачи, но не влиять на сам замер. При этом, если мы оставим под код самой задачи какие-то заглушки, компилятор может выбросить их или связанные с ними вычисления полностью.

Думаете это все? 

Нет! Тест, который вы написали не будет вообще никак соотноситься с реальностью из-за того, что тест крутиться в цикле и каждая итерация следует сразу за предыдущей и это дает JIT’у, GC, да и ОС в целом эффективнее работать с памятью. Правильнее делать тесты так, чтобы присутствовали перерывы в виде какой-то работы между запусками. Какая продолжительность перерывов и какую работу нужно сделать? Вам никто не скажет! Конечно, можно позапускать среду исполнения с определенным флагами и дергать GC вручную, но этого недостаточно и не соответствует реальной работе. Я в [своем тесте](https://t.me/reatom_ru_news/5) делю каждую итерацию одного теста итерацией других тестов.

Уверен, что упомянул не все существующие моменты, но общая картина у вас, надеюсь, сложилась. Невозможно написать абсолютно верный с формальной точки зрения тест производительности. Вы можете приблизиться к этому потратив несоразмерно много сил и времени, но зачем? Напомню, что подобные тесты нужны для помощи в какой-то общей аналитики курса(!) разработки библиотеки: все драматически медленно или норм. Какие-то уточнения и погони за десятками, а не сотнями, процентов - уже излишки для продуктивности и относятся скорее к хобби или утолению собственного эго, хотя бывают и какие-то специфические случаи для крупных заказчиков.

Конечно, лучше всего мерить производительность кода на реальных задачах (вы ждали этой фразы?), но когда библиотека решает наишироченный спектр задач, например, как стейт менеджер, невозможно сделать релевантный сразу для всех случаев тест, или даже несколько тестов.

В итоге, я выбрал следующие условия измерения: несколько запусков с разным количеством итераций, разбивка каждой итерации итерациями других тестов, медианное значение, средняя (из головы) сложность тестируемого кода и сложение интеджеров внутри тестируемой задачи.



Немного контекста:

- [Разработчик V8 про перфмерилк](https://youtu.be/HPFARivHJRY)и (часовое видео с докладом)
- [Его же блог](https://mrale.ph/)

